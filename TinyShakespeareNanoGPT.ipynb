{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0oshdb+STzNBFxfqFx66V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niral28/vibe-coding-course/blob/main/TinyShakespeareNanoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NanoGPT Learning Exercise: From Pre-training to RLHF\n",
        "## Educational notebook for understanding the complete LLM training pipeline\n",
        "\n",
        "By: Niral Shah\n",
        "GSET Vibe Coding: AI Remix\n",
        "\n",
        "\n",
        "Learning Objectives:\n",
        "1. Understand data preprocessing and tokenization\n",
        "2. Implement a simple GPT model from scratch\n",
        "3. Train the model using next-token prediction (pre-training)\n",
        "4. Apply Supervised Fine-Tuning (SFT) for instruction following\n",
        "5. Implement basic Reinforcement Learning from Human Feedback (RLHF)\n"
      ],
      "metadata": {
        "id": "hl3xD36ZBiEu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1kwJ6dsBEIX",
        "outputId": "145c997a-0fd7-4a36-997a-a0b786297c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Welcome to the NanoGPT Learning Exercise!\n",
            "We'll build a GPT model from scratch and take it through the full training pipeline.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import random\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "np.random.seed(1337)\n",
        "random.seed(1337)\n",
        "\n",
        "print(\"üöÄ Welcome to the NanoGPT Learning Exercise!\")\n",
        "print(\"We'll build a GPT model from scratch and take it through the full training pipeline.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =============================================================================\n",
        "# SECTION 1: DATA PREPARATION AND TOKENIZATION\n"
      ],
      "metadata": {
        "id": "X_bju5K_BYZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 1: DATA PREPARATION AND TOKENIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Download the tiny shakespeare dataset\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "if not os.path.exists('input.txt'):\n",
        "    print(\"üì• Downloading Tiny Shakespeare dataset...\")\n",
        "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    urllib.request.urlretrieve(url, 'input.txt')\n",
        "    print(\"‚úÖ Dataset downloaded!\")\n",
        "\n",
        "# Read the dataset\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C22iWUMbBTRT",
        "outputId": "a4f7d3c1-b421-4517-e0c0-6a5615d9144a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SECTION 1: DATA PREPARATION AND TOKENIZATION\n",
            "============================================================\n",
            "üì• Downloading Tiny Shakespeare dataset...\n",
            "‚úÖ Dataset downloaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"üìä Dataset size: {len(text):,} characters\")\n",
        "print(f\"üìñ First 500 characters:\")\n",
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHt7sfayB5U6",
        "outputId": "724c009f-2317-45eb-ca27-882aa334e3da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Dataset size: 1,115,394 characters\n",
            "üìñ First 500 characters:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE 1: Character-level tokenization\n",
        "print(\"\\nüî§ EXERCISE 1: Understanding Tokenization\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Get unique characters and create vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Vocabulary: {''.join(chars)}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYKQnIcyCVy-",
        "outputId": "debe82d9-329e-4507-ba45-33c371709097"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üî§ EXERCISE 1: Understanding Tokenization\n",
            "----------------------------------------\n",
            "Vocabulary: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Vocabulary size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create character mappings\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}  # string to integer\n",
        "itos = {i: ch for i, ch in enumerate(chars)}  # integer to string\n",
        "\n",
        "# Encoding and decoding functions\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Test encoding/decoding\n",
        "test_string = \"hello world\"\n",
        "encoded = encode(test_string)\n",
        "decoded = decode(encoded)\n",
        "print(f\"Original: '{test_string}'\")\n",
        "print(f\"Encoded: {encoded}\")\n",
        "print(f\"Decoded: '{decoded}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oiESTuMCc39",
        "outputId": "91948484-f5e2-4132-953e-599f5c1980e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: 'hello world'\n",
            "Encoded: [46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
            "Decoded: 'hello world'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STUDENT EXERCISE: Try encoding your own string\n",
        "print(\"\\n‚úèÔ∏è YOUR TURN: Try encoding your name or a short phrase!\")"
      ],
      "metadata": {
        "id": "B6fVzB1ECjiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your_string = \"your name here\"  # Replace with your string\n",
        "# encoded_result = encode(your_string)\n",
        "# print(f\"Your string encoded: {encoded_result}\")\n",
        "\n",
        "# Convert entire dataset to tokens\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(f\"\\nüìà Tokenized dataset shape: {data.shape}\")\n",
        "\n",
        "# Train/validation split\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(f\"Training tokens: {len(train_data):,}\")\n",
        "print(f\"Validation tokens: {len(val_data):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff0LsyrFCQOo",
        "outputId": "29f22092-1b6f-423f-ba76-c3043876e118"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìà Tokenized dataset shape: torch.Size([1115394])\n",
            "Training tokens: 1,003,854\n",
            "Validation tokens: 111,540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 2: UNDERSTANDING CONTEXT WINDOWS AND BATCHING"
      ],
      "metadata": {
        "id": "R2mbVRfBCwvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 2: CONTEXT WINDOWS AND BATCHING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Hyperparameters\n",
        "block_size = 8  # Maximum context length\n",
        "batch_size = 4  # Number of sequences processed in parallel\n",
        "\n",
        "def get_batch(split):\n",
        "    \"\"\"Generate a batch of data for training or validation.\"\"\"\n",
        "    data_source = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data_source) - block_size, (batch_size,))\n",
        "    x = torch.stack([data_source[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data_source[i + 1:i + block_size + 1] for i in ix])\n",
        "    return x, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc0KTlgQCC-d",
        "outputId": "e5ba9266-3a23-4c93-9257-32b3a40ce535"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SECTION 2: CONTEXT WINDOWS AND BATCHING\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE 2: Understanding the training setup\n",
        "print(\"\\nüéØ EXERCISE 2: Understanding Context and Targets\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print(f\"Input batch shape: {xb.shape}\")\n",
        "print(f\"Target batch shape: {yb.shape}\")\n",
        "print(f\"\\nExample batch:\")\n",
        "print(f\"Inputs (x): {xb}\")\n",
        "print(f\"Targets (y): {yb}\")\n",
        "\n",
        "print(\"\\nüìö Understanding the training pairs:\")\n",
        "for b in range(min(2, batch_size)):  # Show first 2 sequences\n",
        "    print(f\"\\nSequence {b + 1}:\")\n",
        "    for t in range(min(4, block_size)):  # Show first 4 positions\n",
        "        context = xb[b, :t + 1]\n",
        "        target = yb[b, t]\n",
        "        context_str = decode(context.tolist())\n",
        "        target_str = itos[target.item()]\n",
        "        print(f\"  Context: '{context_str}' ‚Üí Target: '{target_str}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE0J22qfC1En",
        "outputId": "b57d4de3-ecc6-4adc-fe35-016e769161e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ EXERCISE 2: Understanding Context and Targets\n",
            "----------------------------------------\n",
            "Input batch shape: torch.Size([4, 8])\n",
            "Target batch shape: torch.Size([4, 8])\n",
            "\n",
            "Example batch:\n",
            "Inputs (x): tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "Targets (y): tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "\n",
            "üìö Understanding the training pairs:\n",
            "\n",
            "Sequence 1:\n",
            "  Context: 'L' ‚Üí Target: 'e'\n",
            "  Context: 'Le' ‚Üí Target: 't'\n",
            "  Context: 'Let' ‚Üí Target: '''\n",
            "  Context: 'Let'' ‚Üí Target: 's'\n",
            "\n",
            "Sequence 2:\n",
            "  Context: 'f' ‚Üí Target: 'o'\n",
            "  Context: 'fo' ‚Üí Target: 'r'\n",
            "  Context: 'for' ‚Üí Target: ' '\n",
            "  Context: 'for ' ‚Üí Target: 't'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 3: BUILDING THE GPT MODEL"
      ],
      "metadata": {
        "id": "7WhNM1ggDQ1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 3: BUILDING THE GPT MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Model hyperparameters\n",
        "n_embd = 64      # Embedding dimension\n",
        "n_head = 4       # Number of attention heads\n",
        "n_layer = 4      # Number of transformer blocks\n",
        "dropout = 0.1    # Dropout rate\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"Single head of self-attention.\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)    # (B, T, head_size)\n",
        "        q = self.query(x)  # (B, T, head_size)\n",
        "\n",
        "        # Compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5  # Scaled attention\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # Apply attention to values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication (attention) + computation (feed-forward).\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Residual connections with layer norm (pre-norm variant)\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    \"\"\"A simple GPT model.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # Final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Get embeddings\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T))  # (T, n_embd)\n",
        "        x = tok_emb + pos_emb  # (B, T, n_embd)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"Generate new tokens autoregressively.\"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop to last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # Get predictions\n",
        "            logits, _ = self(idx_cond)\n",
        "            # Focus on last time step\n",
        "            logits = logits[:, -1, :]  # (B, C)\n",
        "            # Apply softmax\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # Sample from distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # Append to sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# Create model\n",
        "model = GPTModel()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"üß† Model created with {total_params/1e6:.2f}M parameters\")\n",
        "\n",
        "# Test the model before training\n",
        "print(\"\\nüé≤ Sample generation before training:\")\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "sample = decode(model.generate(context, max_new_tokens=100)[0].tolist())\n",
        "print(sample[:200] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wwFtNbACrox",
        "outputId": "43db7540-0745-4d80-95a5-04c0da82d69a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SECTION 3: BUILDING THE GPT MODEL\n",
            "============================================================\n",
            "üß† Model created with 0.21M parameters\n",
            "\n",
            "üé≤ Sample generation before training:\n",
            "\n",
            ";vFNswf,x&SHwQKgdu;TpS,aJTRl$ZxD&An'lTc$Ala!BQYkeYwJYgm:xr,O:YVhnIYOtfSa$,A,!UUGyIvfykuxR-,TR;UbcQ!Z...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 4: PRE-TRAINING (NEXT TOKEN PREDICTION)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 4: PRE-TRAINING (NEXT TOKEN PREDICTION)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Training hyperparameters\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 200\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"Estimate loss on train and validation sets.\"\"\"\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aeujE42Dplb",
        "outputId": "0abe9f4b-d8bd-4a12-9ea2-496431d133ab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SECTION 4: PRE-TRAINING (NEXT TOKEN PREDICTION)\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üöÇ Starting pre-training...\")\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # Evaluate periodically\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        train_losses.append(losses['train'])\n",
        "        val_losses.append(losses['val'])\n",
        "        print(f\"Step {iter:4d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # Training step\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"‚úÖ Pre-training completed!\")\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Evaluation Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Pre-training Loss Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Generate sample after pre-training\n",
        "print(\"\\nüìù Sample generation after pre-training:\")\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "sample = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "69oCqpzdCrtr",
        "outputId": "dbba4369-62a3-4387-8134-93b2223b1dab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÇ Starting pre-training...\n",
            "Step    0: train loss 4.2385, val loss 4.2494\n",
            "Step  300: train loss 2.8896, val loss 2.9064\n",
            "Step  600: train loss 2.6516, val loss 2.6730\n",
            "Step  900: train loss 2.5817, val loss 2.5549\n",
            "Step 1200: train loss 2.5175, val loss 2.4929\n",
            "Step 1500: train loss 2.4642, val loss 2.4785\n",
            "Step 1800: train loss 2.4328, val loss 2.4402\n",
            "Step 2100: train loss 2.4187, val loss 2.4165\n",
            "Step 2400: train loss 2.3751, val loss 2.3783\n",
            "Step 2700: train loss 2.3549, val loss 2.3827\n",
            "Step 2999: train loss 2.3423, val loss 2.3463\n",
            "‚úÖ Pre-training completed!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Sample generation after pre-training:\n",
            "\n",
            "\n",
            "CES:\n",
            "And Me blitout ham hamanot ha, wuuch oud fome pay, to minvershed,\n",
            "Ith wy tout neas y worig hBXAndt and wowenee thoce iselce.\n",
            "\n",
            "Water, sor wed son hit ing bethou I thally hea rems ited, st ald itefs thof sorst'des poreakedpllel\n",
            "Thar yougenlke stitene the rom\n",
            "\n",
            "ipase lishy:\n",
            "Muik nout he anotweedllt paour wyat,\n",
            "He.\n",
            "\n",
            "RIIAQINC:\n",
            "AR!KDOMUEO\n",
            "CHINBENThow mee there spea lir:\n",
            "Witley'fkincisto mat tous For, keve he ay amy ue aorof eqis ar feros ashem,foes no med mereerpe Ees lor, aknof mily coud:\n",
            "Ead ha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 5: SUPERVISED FINE-TUNING (SFT)"
      ],
      "metadata": {
        "id": "7TQsqfKFEHWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 5: SUPERVISED FINE-TUNING (SFT)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"üéØ Now we'll fine-tune our model to follow instructions!\")\n",
        "\n",
        "# Create instruction-following dataset\n",
        "def create_instruction_dataset():\n",
        "    \"\"\"Create a simple instruction-following dataset.\"\"\"\n",
        "    instructions = [\n",
        "        {\n",
        "            \"instruction\": \"Write a short poem about nature.\",\n",
        "            \"response\": \"The trees sway gently in the breeze,\\nWhile flowers dance among the leaves.\\nNature's beauty brings us peace,\\nA symphony that never cease.\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Explain what happiness means.\",\n",
        "            \"response\": \"Happiness is a warm feeling of joy and contentment. It comes from meaningful relationships, personal growth, and finding purpose in life.\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Tell me about the ocean.\",\n",
        "            \"response\": \"The ocean is vast and deep, covering most of Earth's surface. It's home to countless creatures and plays a vital role in our climate.\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"How do you make friends?\",\n",
        "            \"response\": \"Making friends involves being kind, showing genuine interest in others, being a good listener, and participating in activities you enjoy.\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"What is learning?\",\n",
        "            \"response\": \"Learning is the process of acquiring new knowledge, skills, or understanding through study, experience, or teaching.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Format as conversational data\n",
        "    formatted_data = []\n",
        "    for item in instructions:\n",
        "        conversation = f\"Human: {item['instruction']}\\n\\nAssistant: {item['response']}\"\n",
        "        formatted_data.append(conversation)\n",
        "\n",
        "    return formatted_data\n",
        "\n",
        "# Create SFT dataset\n",
        "sft_data = create_instruction_dataset()\n",
        "print(f\"üìö Created {len(sft_data)} instruction examples\")\n",
        "print(\"\\nExample conversation:\")\n",
        "print(sft_data[0])\n",
        "\n",
        "class SFTDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, conversations, block_size):\n",
        "        self.conversations = conversations\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # Tokenize all conversations\n",
        "        self.tokenized_data = []\n",
        "        for conv in conversations:\n",
        "            tokens = encode(conv)\n",
        "            if len(tokens) > block_size:\n",
        "                tokens = tokens[:block_size]\n",
        "            self.tokenized_data.append(tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.tokenized_data[idx]\n",
        "\n",
        "        # Pad if necessary\n",
        "        if len(tokens) < self.block_size:\n",
        "            tokens = tokens + [0] * (self.block_size - len(tokens))\n",
        "\n",
        "        x = torch.tensor(tokens[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(tokens[1:], dtype=torch.long)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "# Create SFT dataset and dataloader\n",
        "sft_dataset = SFTDataset(sft_data * 50, block_size)  # Repeat data for more training\n",
        "sft_dataloader = DataLoader(sft_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "print(f\"üìä SFT dataset size: {len(sft_dataset)} examples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6izXIuOaD-73",
        "outputId": "9e67796a-4d08-40cf-c961-ab3acfae6545"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SECTION 5: SUPERVISED FINE-TUNING (SFT)\n",
            "============================================================\n",
            "üéØ Now we'll fine-tune our model to follow instructions!\n",
            "üìö Created 5 instruction examples\n",
            "\n",
            "Example conversation:\n",
            "Human: Write a short poem about nature.\n",
            "\n",
            "Assistant: The trees sway gently in the breeze,\n",
            "While flowers dance among the leaves.\n",
            "Nature's beauty brings us peace,\n",
            "A symphony that never cease.\n",
            "üìä SFT dataset size: 250 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model\n",
        "print(\"\\nüéØ Starting supervised fine-tuning...\")\n",
        "\n",
        "# Lower learning rate for fine-tuning\n",
        "sft_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "sft_epochs = 5\n",
        "\n",
        "model.train()\n",
        "for epoch in range(sft_epochs):\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(sft_dataloader):\n",
        "        # Forward pass\n",
        "        logits, loss = model(x, y)\n",
        "\n",
        "        # Backward pass\n",
        "        sft_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        sft_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        if batch_idx % 20 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{sft_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Epoch {epoch+1} completed. Average loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"‚úÖ Supervised fine-tuning completed!\")\n",
        "\n",
        "# Test the fine-tuned model\n",
        "print(\"\\nü§ñ Testing fine-tuned model:\")\n",
        "test_prompts = [\n",
        "    \"Human: What is friendship?\\n\\nAssistant:\",\n",
        "    \"Human: Write a haiku about rain.\\n\\nAssistant:\",\n",
        "    \"Human: How do plants grow?\\n\\nAssistant:\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    prompt_tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        response = model.generate(prompt_tokens, max_new_tokens=100)\n",
        "        full_response = decode(response[0].tolist())\n",
        "\n",
        "    print(f\"Response: {full_response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vALRYxuBEREW",
        "outputId": "a5b0a5f6-262c-4921-efe4-d5dc4fac4dc8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ Starting supervised fine-tuning...\n",
            "Epoch 1/5, Batch 0, Loss: 3.4511\n",
            "Epoch 1/5, Batch 20, Loss: 1.6651\n",
            "Epoch 1/5, Batch 40, Loss: 0.9294\n",
            "Epoch 1/5, Batch 60, Loss: 0.5777\n",
            "Epoch 1 completed. Average loss: 1.4621\n",
            "Epoch 2/5, Batch 0, Loss: 0.5410\n",
            "Epoch 2/5, Batch 20, Loss: 0.4831\n",
            "Epoch 2/5, Batch 40, Loss: 0.3682\n",
            "Epoch 2/5, Batch 60, Loss: 0.3251\n",
            "Epoch 2 completed. Average loss: 0.4415\n",
            "Epoch 3/5, Batch 0, Loss: 0.3537\n",
            "Epoch 3/5, Batch 20, Loss: 0.3058\n",
            "Epoch 3/5, Batch 40, Loss: 0.3873\n",
            "Epoch 3/5, Batch 60, Loss: 0.3109\n",
            "Epoch 3 completed. Average loss: 0.3290\n",
            "Epoch 4/5, Batch 0, Loss: 0.3126\n",
            "Epoch 4/5, Batch 20, Loss: 0.3387\n",
            "Epoch 4/5, Batch 40, Loss: 0.2534\n",
            "Epoch 4/5, Batch 60, Loss: 0.3047\n",
            "Epoch 4 completed. Average loss: 0.2850\n",
            "Epoch 5/5, Batch 0, Loss: 0.3236\n",
            "Epoch 5/5, Batch 20, Loss: 0.3060\n",
            "Epoch 5/5, Batch 40, Loss: 0.2881\n",
            "Epoch 5/5, Batch 60, Loss: 0.2905\n",
            "Epoch 5 completed. Average loss: 0.2651\n",
            "‚úÖ Supervised fine-tuning completed!\n",
            "\n",
            "ü§ñ Testing fine-tuned model:\n",
            "\n",
            "==================================================\n",
            "Prompt: Human: What is friendship?\n",
            "\n",
            "Assistant:\n",
            "Response: Human: What is friendship?\n",
            "\n",
            "Assistant:e man: E Euman: WErnE, Whduman: foTH Wquman: Hvuman: ENan: WHu. chan: Wu, My: Whan: Woud an: TEzL: W\n",
            "\n",
            "==================================================\n",
            "Prompt: Human: Write a haiku about rain.\n",
            "\n",
            "Assistant:\n",
            "Response: Human: Write a haiku about rain.\n",
            "\n",
            "Assistant: an: TEO: uran: HoutAn: Thumar WHE: EFCuman: THuman: The bus VEEN: VWhI HEN$\n",
            "Than: couman:\n",
            "MEETHE:\n",
            "W\n",
            "\n",
            "==================================================\n",
            "Prompt: Human: How do plants grow?\n",
            "\n",
            "Assistant:\n",
            "Response: Human: How do plants grow?\n",
            "\n",
            "Assistant: fun: ES: TWUS: Thu, man: Wan: fuman: HAMthin: ENout HouAn: HHan: Eha Human: Human:e TENroENME'F: E:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 6: REINFORCEMENT LEARNING FROM HUMAN FEEDBACK (RLHF)"
      ],
      "metadata": {
        "id": "eR2NeeIfE5P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 6: REINFORCEMENT LEARNING FROM HUMAN FEEDBACK (RLHF)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"üèÜ Now we'll implement a simplified version of RLHF!\")\n",
        "\n",
        "class SimpleRewardModel(nn.Module):\n",
        "    \"\"\"A simple reward model that scores responses.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, n_embd):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # Simple approach: average embeddings and predict reward\n",
        "        embeddings = self.embedding(tokens)\n",
        "        pooled = embeddings.mean(dim=1)  # Average pooling\n",
        "        reward = self.layers(pooled)\n",
        "        return reward.squeeze()\n",
        "\n",
        "# Create reward model\n",
        "reward_model = SimpleRewardModel(vocab_size, n_embd)\n",
        "print(\"üèÖ Reward model created!\")\n",
        "\n",
        "# Simulate human preferences (in practice, this would come from human raters)\n",
        "def simulate_human_preference(response1, response2):\n",
        "    \"\"\"Simulate human preference between two responses.\"\"\"\n",
        "    # Simple heuristic: prefer longer, more varied responses\n",
        "    print(f'Which response do you like better:\\nA) {response1}\\n\\n B){response2}\\n')\n",
        "    preference = input(\"Enter 'A' or 'B': \").strip().upper()\n",
        "    if preference.strip() == 'A':\n",
        "        return 1\n",
        "    else:\n",
        "      return 0\n",
        "    return 1 if score1 > score2 else 0\n",
        "\n",
        "# Generate preference data\n",
        "print(\"\\nüìä Generating preference data...\")\n",
        "preference_data = []\n",
        "\n",
        "prompts = [\n",
        "    \"Human: Tell me about the stars.\\n\\nAssistant:\",\n",
        "    \"Human: What makes a good friend?\\n\\nAssistant:\",\n",
        "    \"Human: Describe a perfect day.\\n\\nAssistant:\"\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    prompt_tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    # Generate two different responses\n",
        "    with torch.no_grad():\n",
        "        response1 = model.generate(prompt_tokens, max_new_tokens=50)\n",
        "        response2 = model.generate(prompt_tokens, max_new_tokens=50)\n",
        "\n",
        "        text1 = decode(response1[0].tolist())\n",
        "        text2 = decode(response2[0].tolist())\n",
        "\n",
        "        # Get human preference\n",
        "        preference = simulate_human_preference(text1, text2)\n",
        "\n",
        "        preference_data.append({\n",
        "            'prompt': prompt,\n",
        "            'response1': text1,\n",
        "            'response2': text2,\n",
        "            'preference': preference  # 1 if response1 preferred, 0 if response2\n",
        "        })\n",
        "\n",
        "print(f\"Generated {len(preference_data)} preference pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tJ3KPVREd9l",
        "outputId": "dfccf963-57c2-45cd-ef7d-aaa372ff0890"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SECTION 6: REINFORCEMENT LEARNING FROM HUMAN FEEDBACK (RLHF)\n",
            "============================================================\n",
            "üèÜ Now we'll implement a simplified version of RLHF!\n",
            "üèÖ Reward model created!\n",
            "\n",
            "üìä Generating preference data...\n",
            "Which response do you like better:\n",
            "A) Human: Tell me about the stars.\n",
            "\n",
            "Assistant: Han: Wumas han: Tun: Whan: WA: EI:'an: Humany Whe\n",
            "\n",
            " B)Human: Tell me about the stars.\n",
            "\n",
            "Assistant: THuman: frdran: Whivinom THaplan: Wrue Whith TENE\n",
            "\n",
            "Enter 'A' or 'B': B\n",
            "Which response do you like better:\n",
            "A) Human: What makes a good friend?\n",
            "\n",
            "Assistant: ThoBum, EE: Wraman: EnCiman: WhuClE min: An: E TH\n",
            "\n",
            " B)Human: What makes a good friend?\n",
            "\n",
            "Assistant:\n",
            "TEwou, Eroun: Whan: WAn: Wyoun: wan: Muman: W; Tu\n",
            "\n",
            "Enter 'A' or 'B': A\n",
            "Which response do you like better:\n",
            "A) Human: Describe a perfect day.\n",
            "\n",
            "Assistant: can: Jun: mann THouman: I Why En: wals KuranLK: E\n",
            "\n",
            " B)Human: Describe a perfect day.\n",
            "\n",
            "Assistant: But ENanKEoz: Human: Hrus CSE: Touge buman: Humou\n",
            "\n",
            "Enter 'A' or 'B': B\n",
            "Generated 3 preference pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train reward model\n",
        "print(\"\\nüéØ Training reward model...\")\n",
        "reward_optimizer = torch.optim.AdamW(reward_model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "\n",
        "    for data in preference_data:\n",
        "        # Tokenize responses\n",
        "        tokens1 = torch.tensor(encode(data['response1']), dtype=torch.long).unsqueeze(0)\n",
        "        tokens2 = torch.tensor(encode(data['response2']), dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        # Get rewards\n",
        "        reward1 = reward_model(tokens1)\n",
        "        reward2 = reward_model(tokens2)\n",
        "\n",
        "        # Bradley-Terry loss\n",
        "        if data['preference'] == 1:\n",
        "            loss = -torch.log(torch.sigmoid(reward1 - reward2))\n",
        "        else:\n",
        "            loss = -torch.log(torch.sigmoid(reward2 - reward1))\n",
        "\n",
        "        reward_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        reward_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        print(f\"Reward model epoch {epoch}, Loss: {total_loss/len(preference_data):.4f}\")\n",
        "\n",
        "print(\"‚úÖ Reward model training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_If4liWQFtR8",
        "outputId": "b3e4fc5c-637d-44e5-f7ec-7fed705f37ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ Training reward model...\n",
            "Reward model epoch 0, Loss: 0.6943\n",
            "Reward model epoch 2, Loss: 0.6812\n",
            "Reward model epoch 4, Loss: 0.6717\n",
            "Reward model epoch 6, Loss: 0.6629\n",
            "Reward model epoch 8, Loss: 0.6531\n",
            "‚úÖ Reward model training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple PPO-style optimization (simplified)\n",
        "print(\"\\nüöÄ Applying reinforcement learning...\")\n",
        "\n",
        "def compute_advantages(rewards, values, gamma=0.99):\n",
        "    \"\"\"Compute advantages for PPO.\"\"\"\n",
        "    advantages = []\n",
        "    for i in range(len(rewards)):\n",
        "        advantage = rewards[i] - values[i]\n",
        "        advantages.append(advantage)\n",
        "    return torch.tensor(advantages)\n",
        "\n",
        "# RL fine-tuning loop (simplified)\n",
        "rl_optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for rl_step in range(20):\n",
        "    # Generate responses\n",
        "    prompt = \"Human: What is the meaning of life?\\n\\nAssistant:\"\n",
        "    prompt_tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        response_tokens = model.generate(prompt_tokens, max_new_tokens=30)\n",
        "\n",
        "    # Get reward\n",
        "    with torch.no_grad():\n",
        "        reward = reward_model(response_tokens)\n",
        "\n",
        "    # Simplified policy gradient step\n",
        "    # Crop response_tokens to block_size before passing to model\n",
        "    cropped_response_tokens = response_tokens[:, -block_size:]\n",
        "    logits, _ = model(cropped_response_tokens[:, :-1], cropped_response_tokens[:, 1:])\n",
        "\n",
        "\n",
        "    # Simple reward-weighted loss\n",
        "    # We need to calculate the loss based on the probability of the generated tokens.\n",
        "    # This is a simplified approach and not a full PPO implementation.\n",
        "    # For this simplified example, we'll use a negative reward as the loss to maximize reward.\n",
        "    loss = -reward.mean()\n",
        "\n",
        "    rl_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    rl_optimizer.step()\n",
        "\n",
        "    if rl_step % 5 == 0:\n",
        "        print(f\"RL step {rl_step}, Reward: {reward.mean().item():.4f}\")\n",
        "\n",
        "print(\"‚úÖ RLHF training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "7KinDEHiFCVJ",
        "outputId": "491af279-cc93-4453-9021-8792ef595998"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Applying reinforcement learning...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-1982906781.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mrl_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mrl_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 7: FINAL EVALUATION"
      ],
      "metadata": {
        "id": "YgFCr7KhEynO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 7: FINAL EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"üéâ Let's see how our model performs after the full training pipeline!\")\n",
        "\n",
        "# Test the final model\n",
        "final_test_prompts = [\n",
        "    \"Human: What is artificial intelligence?\\n\\nAssistant:\",\n",
        "    \"Human: Write a story about a robot.\\n\\nAssistant:\",\n",
        "    \"Human: How can I be more creative?\\n\\nAssistant:\",\n",
        "    \"Human: What is the purpose of education?\\n\\nAssistant:\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(final_test_prompts):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TEST {i+1}\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    prompt_tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        response = model.generate(prompt_tokens, max_new_tokens=150)\n",
        "        full_response = decode(response[0].tolist())\n",
        "\n",
        "        # Get reward score\n",
        "        reward_score = reward_model(response)\n",
        "\n",
        "    print(f\"Response: {full_response}\")\n",
        "    print(f\"Reward Score: {reward_score.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y83wWiMiEvJn",
        "outputId": "f701d894-9a09-4fd0-f34f-bc215d85c57a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SECTION 7: FINAL EVALUATION\n",
            "============================================================\n",
            "üéâ Let's see how our model performs after the full training pipeline!\n",
            "\n",
            "============================================================\n",
            "TEST 1\n",
            "Prompt: Human: What is artificial intelligence?\n",
            "\n",
            "Assistant:\n",
            "Response: Human: What is artificial intelligence?\n",
            "\n",
            "Assistant: Mushan tEhan: Human WER: froJan: Houman: an: Hurn Burstingl: Tan: Ewan: youm: of wouman: Whobe youceas W: anue mann: Tuman: Whcriduman: ENuman: T: Wr\n",
            "Reward Score: -0.2289\n",
            "\n",
            "============================================================\n",
            "TEST 2\n",
            "Prompt: Human: Write a story about a robot.\n",
            "\n",
            "Assistant:\n",
            "Response: Human: Write a story about a robot.\n",
            "\n",
            "Assistant: W!f WKEEwAU: E: TTer Wumeyn: Cithum Tun: Human: CHe EERE: MuH HaBumanT: Tin: Wrun: Jhaman: uman: WFen T.\n",
            "WE!\n",
            "WERE EfAn: I Yown: Wiman: Mucoun: Thann:\n",
            "Reward Score: -0.1968\n",
            "\n",
            "============================================================\n",
            "TEST 3\n",
            "Prompt: Human: How can I be more creative?\n",
            "\n",
            "Assistant:\n",
            "Response: Human: How can I be more creative?\n",
            "\n",
            "Assistant: Whrut: Whuman: E: TEEN: doumann HEN: Whean: Thallan: TEO: Human: ;omuprand anunumerll hothumac: VIN houman: ESnoumman: TE, TMus ToucEd TKA:\n",
            "Whan: Hum\n",
            "Reward Score: -0.1872\n",
            "\n",
            "============================================================\n",
            "TEST 4\n",
            "Prompt: Human: What is the purpose of education?\n",
            "\n",
            "Assistant:\n",
            "Response: Human: What is the purpose of education?\n",
            "\n",
            "Assistant: fren: TRouman: EnRC: WUWhRYods Thin: ENuman: I WHan: W$URoEV: EBun: duman: Fhan WCETEE; AO!EUwann: Ein: Wuman:: of EGboman: EMun: HEMuman: HAn: THump\n",
            "Reward Score: -0.1702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 8: EXERCISES AND EXTENSIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 8: EXERCISES FOR STUDENTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "exercise_prompts = \"\"\"\n",
        "üéØ EXERCISES TO TRY:\n",
        "\n",
        "1. TOKENIZATION EXPERIMENTS:\n",
        "   - Try different tokenization strategies (word-level, subword)\n",
        "   - Implement BPE (Byte Pair Encoding) tokenization\n",
        "   - Compare vocabulary sizes and model performance\n",
        "\n",
        "2. MODEL ARCHITECTURE:\n",
        "   - Experiment with different model sizes (n_embd, n_layer, n_head)\n",
        "   - Add more sophisticated positional encodings\n",
        "   - Implement rotary positional embeddings (RoPE)\n",
        "\n",
        "3. TRAINING IMPROVEMENTS:\n",
        "   - Implement learning rate scheduling\n",
        "   - Add gradient clipping\n",
        "   - Try different optimizers (Adam, AdamW, Lion)\n",
        "\n",
        "4. DATA AUGMENTATION:\n",
        "   - Create more diverse instruction-following data\n",
        "   - Implement data augmentation techniques\n",
        "   - Add different types of tasks (summarization, QA, etc.)\n",
        "\n",
        "5. REWARD MODEL ENHANCEMENTS:\n",
        "   - Build a more sophisticated reward model\n",
        "   - Implement multiple reward criteria (helpfulness, harmlessness, honesty)\n",
        "   - Add constitutional AI principles\n",
        "\n",
        "6. RLHF IMPROVEMENTS:\n",
        "   - Implement proper PPO (Proximal Policy Optimization)\n",
        "   - Add KL divergence penalty to prevent mode collapse\n",
        "   - Implement rejection sampling and best-of-N sampling\n",
        "\n",
        "7. EVALUATION:\n",
        "   - Implement automatic evaluation metrics\n",
        "   - Create human evaluation interfaces\n",
        "   - Compare models at different training stages\n",
        "\n",
        "8. SCALING:\n",
        "   - Train on larger datasets\n",
        "   - Implement distributed training\n",
        "   - Experiment with model parallelism\n",
        "\n",
        "üí° RESEARCH QUESTIONS:\n",
        "- How does model size affect the quality of instruction following?\n",
        "- What's the optimal ratio of pre-training to fine-tuning data?\n",
        "- How sensitive is RLHF to the quality of the reward model?\n",
        "- Can you implement constitutional AI or other alignment techniques?\n",
        "\"\"\"\n",
        "\n",
        "print(exercise_prompts)\n",
        "\n",
        "print(\"\\nüéì CONGRATULATIONS!\")\n",
        "print(\"You've successfully built a GPT model from scratch and taken it through\")\n",
        "print(\"the complete training pipeline: Pre-training ‚Üí SFT ‚Üí RLHF\")\n",
        "print(\"\\nKey takeaways:\")\n",
        "print(\"1. Pre-training teaches the model language and world knowledge\")\n",
        "print(\"2. SFT teaches the model to follow instructions and be helpful\")\n",
        "print(\"3. RLHF aligns the model with human preferences and values\")\n",
        "print(\"\\nNow go forth and build amazing AI systems! üöÄ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhZpSDdSExTG",
        "outputId": "8a394b74-f718-4d18-e02c-56a59ab4c347"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SECTION 8: EXERCISES FOR STUDENTS\n",
            "============================================================\n",
            "\n",
            "üéØ EXERCISES TO TRY:\n",
            "\n",
            "1. TOKENIZATION EXPERIMENTS:\n",
            "   - Try different tokenization strategies (word-level, subword)\n",
            "   - Implement BPE (Byte Pair Encoding) tokenization\n",
            "   - Compare vocabulary sizes and model performance\n",
            "\n",
            "2. MODEL ARCHITECTURE:\n",
            "   - Experiment with different model sizes (n_embd, n_layer, n_head)\n",
            "   - Add more sophisticated positional encodings\n",
            "   - Implement rotary positional embeddings (RoPE)\n",
            "\n",
            "3. TRAINING IMPROVEMENTS:\n",
            "   - Implement learning rate scheduling\n",
            "   - Add gradient clipping\n",
            "   - Try different optimizers (Adam, AdamW, Lion)\n",
            "\n",
            "4. DATA AUGMENTATION:\n",
            "   - Create more diverse instruction-following data\n",
            "   - Implement data augmentation techniques\n",
            "   - Add different types of tasks (summarization, QA, etc.)\n",
            "\n",
            "5. REWARD MODEL ENHANCEMENTS:\n",
            "   - Build a more sophisticated reward model\n",
            "   - Implement multiple reward criteria (helpfulness, harmlessness, honesty)\n",
            "   - Add constitutional AI principles\n",
            "\n",
            "6. RLHF IMPROVEMENTS:\n",
            "   - Implement proper PPO (Proximal Policy Optimization)\n",
            "   - Add KL divergence penalty to prevent mode collapse\n",
            "   - Implement rejection sampling and best-of-N sampling\n",
            "\n",
            "7. EVALUATION:\n",
            "   - Implement automatic evaluation metrics\n",
            "   - Create human evaluation interfaces\n",
            "   - Compare models at different training stages\n",
            "\n",
            "8. SCALING:\n",
            "   - Train on larger datasets\n",
            "   - Implement distributed training\n",
            "   - Experiment with model parallelism\n",
            "\n",
            "üí° RESEARCH QUESTIONS:\n",
            "- How does model size affect the quality of instruction following?\n",
            "- What's the optimal ratio of pre-training to fine-tuning data?\n",
            "- How sensitive is RLHF to the quality of the reward model?\n",
            "- Can you implement constitutional AI or other alignment techniques?\n",
            "\n",
            "\n",
            "üéì CONGRATULATIONS!\n",
            "You've successfully built a GPT model from scratch and taken it through\n",
            "the complete training pipeline: Pre-training ‚Üí SFT ‚Üí RLHF\n",
            "\n",
            "Key takeaways:\n",
            "1. Pre-training teaches the model language and world knowledge\n",
            "2. SFT teaches the model to follow instructions and be helpful\n",
            "3. RLHF aligns the model with human preferences and values\n",
            "\n",
            "Now go forth and build amazing AI systems! üöÄ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "omvCO4ztGP7C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}